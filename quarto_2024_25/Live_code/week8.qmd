---
title: Week 8 practice
format: live-html
engine: knitr
webr:
  packages:
    - wordcloud2
    - tm
    - ggplot2
---
## Text analytics

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

In **Week 8 Workshop** we are using text analytics and classification models to predict whether YouTube comments are HAM or SPAM.

## Word Clouds

A very simple way of analysing text is to count occurances of words.
We see word clouds a lot!
How useful are they?

Look at the data cleaning steps required.

First, load your data:

::: {.callout-tip title="Load in your 'corpus' or document" collapse="true"}
```{webr}
text <- c("To understand what business analytics is, 
itâ€™s also important to distinguish it from data science. 
While both processes analyze data to solve business problems, 
the difference between business analytics and data science lies
in how data is used. Business analytics is concerned with 
extracting meaningful insights from and visualizing data to 
facilitate the decision-making process, whereas data science 
is focused on making sense of raw data using algorithms, 
statistical models, and computer programming. Despite their 
differences, both business analytics and data science glean 
insights from data to inform business decisions. To better 
understand how data insights can drive organizational performance,
here are some of the ways firms have benefitted from using 
business analytics.")
```
:::

```{webr}

# Turn off warnings
options(warn = -1)
# Create a text corpus
corpus <- Corpus(VectorSource(text))

# Preprocess the text: remove punctuation, numbers, whitespace, and convert to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Create a term-document matrix
tdm <- TermDocumentMatrix(corpus)

# Convert the matrix to a data frame
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing = TRUE) 
df <- data.frame(word = names(word_freqs), freq = word_freqs)

# Set up the plot area with a white background
par(bg = "white", mar = c(0,0,0,0))  # Set the margins to zero

# Suppose 'df' is your data frame with columns 'word' and 'freq'
wordcloud2(df,
           size = 1,
           minRotation = -pi/4,  # -45 degrees
           maxRotation = pi/4,   # +45 degrees
           rotateRatio = 0.5)    # 50% of words get rotated within this range

```

## Is this more useful?

```{webr}
#extract the top 15 words
top_n <- 15
df_top <- head(df, top_n)

# Ensure it's sorted by frequency for a descending bar chart
df_top <- df_top[order(df_top$freq, decreasing = TRUE), ]

# Convert 'word' into a factor that respects this order
df_top$word <- factor(df_top$word, levels = df_top$word)

# Plot
ggplot(df_top, aes(x = word, y = freq)) +
  geom_bar(stat = "identity", fill="springgreen3") +
  coord_flip() +  # Rotate so words read nicely (optional)
  labs(title = "Top 15 Words",
       x = "Word",
       y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))


```

## Could you improve it further?
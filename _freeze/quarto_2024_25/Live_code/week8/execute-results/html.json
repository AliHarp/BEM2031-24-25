{
  "hash": "e7a4ae1ccb0bb358106bd4c570734a88",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Week 8 practice\nformat: live-html\nengine: knitr\nwebr:\n  packages:\n    - wordcloud2\n    - tm\n    - ggplot2\n---\n\n## Text analytics\n\n::: {.cell}\n\n:::\n\n\n\nIn **Week 8 Workshop** we are using text analytics and classification models to predict whether YouTube comments are HAM or SPAM.\n\n## Word Clouds\n\nA very simple way of analysing text is to count occurances of words.\nWe see word clouds a lot!\nHow useful are they?\n\nLook at the data cleaning steps required.\n\nFirst, load your data:\n\n::: {.callout-tip title=\"Load in your 'corpus' or document\" collapse=\"true\"}\n\n::: {.cell}\n```{webr}\ntext <- c(\"To understand what business analytics is, \nitâ€™s also important to distinguish it from data science. \nWhile both processes analyze data to solve business problems, \nthe difference between business analytics and data science lies\nin how data is used. Business analytics is concerned with \nextracting meaningful insights from and visualizing data to \nfacilitate the decision-making process, whereas data science \nis focused on making sense of raw data using algorithms, \nstatistical models, and computer programming. Despite their \ndifferences, both business analytics and data science glean \ninsights from data to inform business decisions. To better \nunderstand how data insights can drive organizational performance,\nhere are some of the ways firms have benefitted from using \nbusiness analytics.\")\n```\n:::\n\n:::\n\n\n::: {.cell}\n```{webr}\n\n# Turn off warnings\noptions(warn = -1)\n# Create a text corpus\ncorpus <- Corpus(VectorSource(text))\n\n# Preprocess the text: remove punctuation, numbers, whitespace, and convert to lowercase\ncorpus <- tm_map(corpus, content_transformer(tolower))\ncorpus <- tm_map(corpus, removePunctuation)\ncorpus <- tm_map(corpus, removeNumbers)\ncorpus <- tm_map(corpus, stripWhitespace)\ncorpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n\n# Create a term-document matrix\ntdm <- TermDocumentMatrix(corpus)\n\n# Convert the matrix to a data frame\nm <- as.matrix(tdm)\nword_freqs <- sort(rowSums(m), decreasing = TRUE) \ndf <- data.frame(word = names(word_freqs), freq = word_freqs)\n\n# Set up the plot area with a white background\npar(bg = \"white\", mar = c(0,0,0,0))  # Set the margins to zero\n\n# Suppose 'df' is your data frame with columns 'word' and 'freq'\nwordcloud2(df,\n           size = 1,\n           minRotation = -pi/4,  # -45 degrees\n           maxRotation = pi/4,   # +45 degrees\n           rotateRatio = 0.5)    # 50% of words get rotated within this range\n\n```\n:::\n\n\n## Is this more useful?\n\n\n::: {.cell}\n```{webr}\n#extract the top 15 words\ntop_n <- 15\ndf_top <- head(df, top_n)\n\n# Ensure it's sorted by frequency for a descending bar chart\ndf_top <- df_top[order(df_top$freq, decreasing = TRUE), ]\n\n# Convert 'word' into a factor that respects this order\ndf_top$word <- factor(df_top$word, levels = df_top$word)\n\n# Plot\nggplot(df_top, aes(x = word, y = freq)) +\n  geom_bar(stat = \"identity\", fill=\"springgreen3\") +\n  coord_flip() +  # Rotate so words read nicely (optional)\n  labs(title = \"Top 15 Words\",\n       x = \"Word\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))\n\n```\n:::\n\n\n## Could you improve it further?",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
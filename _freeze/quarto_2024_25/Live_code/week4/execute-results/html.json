{
  "hash": "3dd9c72951d4e63743e08c2b6ce86ea2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Week 4 practice\nformat: live-html\nengine: knitr\nwebr:\n  packages:\n    - ggplot2\n    - factoextra\n---\n\n## Clusters and similarity\n\n::: {.cell}\n\n:::\n\n\n\nIn **Week 4 Workshop** we are focusing on clustering and dimension reduction.\n\n## k-means clustering\n\nThe code sets a seed for reproducibility as we are randomly sampling from the `runif` (random uniform) and `rnorm` (random normal) distributions to generate a dataset of `k` clusters.\n\nWe define parameters for number of clusters `k` and number of points per cluster `N`. We randomly pick cluster centres `xs` and `xy`. For each centre, we generate normally distributed data points around those centres, with a standard deviation of 0.5.\n\nTry changing `k` with the same seed.\nTry changing the seed.\nWhat happens?\n\n\n::: {.cell}\n```{webr}\n#set seed so we get the same results each time\nset.seed(666)\n\n# Parameters\nk <- 5 # how many clusters do we want?\nN <- 500\n\n# Generate cluster centers\nxs <- runif(k, 0, 10)\nys <- runif(k, 0, 10)\n\n# Generate data for each cluster\nd <- data.frame(\n  xs = unlist(lapply(xs, function(x) rnorm(N, mean = x, sd = 0.5))),\n  ys = unlist(lapply(ys, function(y) rnorm(N, mean = y, sd = 0.5))),\n  cluster = factor(rep(1:k, each = N)) # Add a 'cluster' column\n)\n\n# Plot the data\nggplot(d, aes(x = xs, y = ys, color = cluster)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  scale_color_discrete(name = \"Cluster\")\n```\n:::\n\n\nSee if the Within Cluster Sum of Squares (WSS) recommends the same number of clusters as those we have randomly generated.\n\nTry changing the method to 'silhouette'. The Silhouette method shows how close each point in a cluster is to points in the neighboring cluster.\n\nAre they the same or different? Try it with different `k` and different seeds.\n\n\n::: {.cell}\n```{webr}\nfviz_nbclust(d,kmeans, method = 'wss')\n```\n:::\n\n\nLet's use the dataset we created `d` and run a kmeans algorithm. What happens? Try it with different `k` and with different seeds.\n\n>We did not set a seed when running the algorithm. If we did, our results would be identical each time we run it (try it, eg `set.seed(123)` before running `kmeans` so the initial cluster centres are chosen reproducibly.\n\n>`kmeans` starts by randomly choosing initial cluster centres unless you have explictly specified an intialisation method. Because of this random intialisation, each run can converge to slightly different final centres and varying cluster assignments. \n\n>A better way of dealing with this is to use an `nstart` parameter to tell `kmeans` how many different random sets of intial centroids to try. It then picks the best solution, i.e. the one with the lowest total within-cluster variation.\nTry adding `nstart = 25` to the `kmeans` function.\n\n\n::: {.cell}\n```{webr}\n# Perform k-means clustering (use only numeric columns)\nkmeans_result <- kmeans(d[, c(\"xs\", \"ys\")], centers = k)\n\n# Visualize clusters using factoextra\nfviz_cluster(kmeans_result, data = d[, c(\"xs\", \"ys\")], geom = \"point\")\n```\n:::\n\n\nHere are a couple of blank code chunks. You could, for example:\n* Inspect `d` (e.g. `head(d)`)  \n* Look at the cluster assignments (`kmeans_result$cluster`)  \n* Look athe the resulting cluster centres (`kmeans_result$centers`)  \n\n\n::: {.cell}\n```{webr}\n\n```\n:::\n\n::: {.cell}\n```{webr}\n\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}